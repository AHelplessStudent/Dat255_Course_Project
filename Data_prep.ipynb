{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data-prep.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data prep and reduction\n",
        "\n",
        "This notebook loads the full data set and selects the wanted features.\n",
        "\n",
        "It also makes a reduced version of the dataset, with a combined and seperate signal files.\n",
        "\n",
        "The default size of the reduced dataset is 10% of the full sized set.\n",
        "\n",
        "### Note:\n",
        "\n",
        "We did not do a lot of \"data exploration\" as the dataset is simulated. So the data should already be \"cleaned\" and distributed. \n",
        "\n",
        "Although we do some exploration of the data in other notebooks    "
      ],
      "metadata": {
        "id": "CngK9tT9jgWa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf-ryjjnEkuj"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tiqjw0EBFmMm",
        "outputId": "b64e5bd8-f9a1-439c-ae6b-eeba7563bb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to appropriate path\n",
        "path = 'drive/My Drive/DAT255 Zprime data/'\n",
        "seed = 42\n",
        "\n",
        "# Reduction variables \n",
        "dst_path = 'drive/MyDrive/Dat255 reduced files/'\n",
        "percentage = 0.1\n",
        "signal_percentage = 0.1\n",
        "\n",
        "\n",
        "#dst_path = 'drive/MyDrive/Dat255 reduced files50/'\n",
        "#percentage = 0.5\n",
        "#signal_percentage = 0.5"
      ],
      "metadata": {
        "id": "KZWE0nGUFsJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gixGInxyk8ne"
      },
      "source": [
        "# These are the final features that were selected during the feature selection process made by Dovydas\n",
        "features_from_feature_importance = [\n",
        "                                    \"met_et\",\n",
        "                                    \"lep_1_E\",\n",
        "                                    \"lep_2_E\",\n",
        "                                    \"lep_3_E\",\n",
        "                                    \"lep_1_eta\",\n",
        "                                    \"lep_2_eta\",\n",
        "                                    \"jet_n\",\n",
        "                                    \"lep_1_pt\",\n",
        "                                    \"lep_2_pt\",\n",
        "                                    \"lep_3_pt\",\n",
        "                                    \"lep_4_pt\",\n",
        "                                    \"lep_5_pt\",\n",
        "                                    \"lep_1_phi\",\n",
        "                                    \"lep_2_phi\",\n",
        "                                    \"jet_2_trueflav\",\n",
        "                                    \"jet_1_E\",\n",
        "                                    \"jet_3_E\",\n",
        "                                    \"jet_1_pt\",\n",
        "                                    \"jet_2_pt\",\n",
        "                                    \"jet_3_pt\",\n",
        "                                    \"jet_4_pt\",\n",
        "                                    \"jet_5_pt\",\n",
        "                                    \"jet_6_pt\",\n",
        "                                    \"jet_7_pt\",\n",
        "                                    \"jet_8_pt\",\n",
        "                                    \"jet_9_pt\",\n",
        "                                    \"alljet_n\",\n",
        "                                    \"lep_1_etcone20\",\n",
        "                                    \"jet_2_MV1\",\n",
        "                                    \"jet_1_MV1\",\n",
        "                                    \"jet_1_phi\",\n",
        "                                    \"jet_1_m\",\n",
        "                                    \"jet_2_E\",\n",
        "                                    \"jet_2_jvf\",\n",
        "                                    \"jet_1_SV0\",\n",
        "                                    ]\n",
        "eventWeights = [\n",
        "\n",
        "                ]\n",
        "\n",
        "# Features should be the same in all files.\n",
        "feature_df     = pd.read_hdf(path + 'mc_110899.ZPrime400.hdf5', 'mini')\n",
        "feature_list = list(feature_df)\n",
        "\n",
        "# Distinguish features from the weights (both are columns in the original file.)\n",
        "for feature in feature_list:\n",
        "  if \"SCALE\" in feature.upper() or \"WEIGHT\" in feature.upper():\n",
        "    eventWeights.append(feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del feature_df"
      ],
      "metadata": {
        "id": "m2HzGGdcc_li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Information needed for calculation the mass of the signal\n",
        "invariant_features = [ 'lep_1_pt',\n",
        "                       'lep_1_eta',\n",
        "                       'lep_1_phi',\n",
        "                       'lep_1_type',\n",
        "                       'lep_1_charge',\n",
        "                       'lep_1_E',\n",
        "                       'lep_2_pt',\n",
        "                       'lep_2_eta',\n",
        "                       'lep_2_phi',\n",
        "                       'lep_2_type',\n",
        "                       'lep_2_charge',\n",
        "                       'lep_2_E',\n",
        "                       'jet_1_pt',\n",
        "                       'jet_1_eta',\n",
        "                       'jet_1_phi',\n",
        "                       'jet_2_pt',\n",
        "                       'jet_2_eta',\n",
        "                       'jet_2_phi',\n",
        "          \n",
        "] "
      ],
      "metadata": {
        "id": "HPVcBXrO7zuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_and_weights = list(set(features_from_feature_importance + invariant_features))\n",
        "features_and_weights += eventWeights"
      ],
      "metadata": {
        "id": "t61iRebq8OtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the background files into dataframes\n",
        "diboson     = pd.read_hdf(path+'diboson.hdf5','mini')[features_and_weights]\n",
        "DYee     = pd.read_hdf(path+'DYee.hdf5','mini')[features_and_weights]\n",
        "DYmumu     = pd.read_hdf( path + 'DYmumu.hdf5', 'mini')[features_and_weights]\n",
        "DYtautau    = pd.read_hdf( path + 'DYtautau.hdf5', 'mini')[features_and_weights]\n",
        "ttbar_lep     = pd.read_hdf( path + 'ttbar_lep.hdf5', 'mini')[features_and_weights]\n",
        "Wenu     = pd.read_hdf( path + 'Wenu.hdf5.hdf5', 'mini')[features_and_weights]\n",
        "Wmunu     = pd.read_hdf( path + 'Wmunu.hdf5', 'mini')[features_and_weights]\n",
        "Wtaunu     = pd.read_hdf( path + 'Wtaunu.hdf5', 'mini')[features_and_weights]\n",
        "\n",
        "\n",
        "## total rows of Zee: 5625000, read in two passes as all the ram was used. \n",
        "Zee1     = pd.read_hdf( path + 'Zee.hdf5', 'mini', stop=2812500)[features_and_weights]\n",
        "Zee2     = pd.read_hdf( path + 'Zee.hdf5', 'mini', start=2812500)[features_and_weights]\n",
        "\n",
        "ttbar_had     = pd.read_hdf( path + 'ttbar_had.hdf5', 'mini')[features_and_weights]\n",
        "Ztautau     = pd.read_hdf( path + 'Ztautau.hdf5', 'mini')[features_and_weights]"
      ],
      "metadata": {
        "id": "IMOphBsjFX3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Zee = pd.concat([Zee1,Zee2])\n",
        "Zee.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfG0PfMyVSBh",
        "outputId": "5f9b202e-e0b5-495e-bc2c-7abb5ef5a426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5625000, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# free up ram\n",
        "del Zee1\n",
        "del Zee2"
      ],
      "metadata": {
        "id": "qSuNN3cYdypc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diboson = shuffle(diboson, random_state = seed)\n",
        "DYee = shuffle(DYee, random_state = seed)\n",
        "DYmumu = shuffle(DYmumu, random_state = seed)\n",
        "DYtautau = shuffle(DYtautau, random_state = seed)\n",
        "ttbar_lep = shuffle(ttbar_lep, random_state = seed)\n",
        "Wenu = shuffle(Wenu, random_state = seed)\n",
        "Wmunu = shuffle(Wmunu, random_state = seed)\n",
        "Wtaunu = shuffle(Wtaunu, random_state = seed)\n",
        "Zee = shuffle(Zee, random_state = seed)\n",
        "ttbar_had = shuffle(ttbar_had, random_state = seed)\n",
        "Ztautau = shuffle(Ztautau, random_state = seed)\n"
      ],
      "metadata": {
        "id": "FPfnGkd3eBAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_file(src_df, name, dst, percentage=0.1):\n",
        "  \"\"\"\n",
        "    src_file: variable of the file you want to reduce\n",
        "    name: name of the file when saved\n",
        "    dst: destination folder\n",
        "    percentage: % of the file you want\n",
        "\n",
        "    data should be shuffeled before reduction\n",
        "  \"\"\"\n",
        "\n",
        "  #new_signal = shuffle(src_file, random_state=seed)\n",
        "  ZprimeX_sample = src_df.sample(frac = percentage, random_state=seed, replace = False) \n",
        "  ZprimeX_sample.to_pickle(dst + name + '.pkl')"
      ],
      "metadata": {
        "id": "Ha2I3XqweAxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V_ASokopeAoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "background_files = [diboson ,\n",
        "                    DYee ,\n",
        "                    DYmumu ,\n",
        "                    DYtautau ,\n",
        "                    ttbar_lep ,\n",
        "                    Wenu ,\n",
        "                    Wmunu ,\n",
        "                    Wtaunu ,\n",
        "                    Zee ,\n",
        "                    ttbar_had ,\n",
        "                    Ztautau ]\n",
        "\n",
        "background_files_names = [\"diboson\" ,\n",
        "                    \"DYee\" ,\n",
        "                    \"DYmumu\" ,\n",
        "                    \"DYtautau\" ,\n",
        "                    \"ttbar_lep\" ,\n",
        "                    \"Wenu\" ,\n",
        "                    \"Wmunu\" ,\n",
        "                    \"Wtaunu\" ,\n",
        "                    \"Zee\" ,\n",
        "                    \"ttbar_had\" ,\n",
        "                    \"Ztautau\" ]\n",
        "\n"
      ],
      "metadata": {
        "id": "PDEh1fb1eAkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(background_files_names)):\n",
        "  reduce_file(background_files[i], background_files_names[i], dst_path, percentage=percentage)"
      ],
      "metadata": {
        "id": "OJTOCRu5eAfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free up ram\n",
        "del diboson \n",
        "del DYee \n",
        "del DYmumu \n",
        "del DYtautau \n",
        "del ttbar_lep \n",
        "del Wenu \n",
        "del Wmunu \n",
        "del Wtaunu \n",
        "del Zee \n",
        "del ttbar_had \n",
        "del Ztautau "
      ],
      "metadata": {
        "id": "V_TsoORTeAac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signal_name_list = [\n",
        "               'mc_110899.ZPrime400.hdf5',\n",
        "               'mc_110901.ZPrime500.hdf5',\n",
        "               'mc_110902.ZPrime750.hdf5',\n",
        "               'mc_110903.ZPrime1000.hdf5',\n",
        "               'mc_110905.ZPrime1500.hdf5',\n",
        "               'mc_110906.ZPrime1750.hdf5',\n",
        "               'mc_110907.ZPrime2000.hdf5',\n",
        "               'mc_110908.ZPrime2250.hdf5',\n",
        "               'mc_110909.ZPrime2500.hdf5',\n",
        "               'mc_110910.ZPrime3000.hdf5'\n",
        "]"
      ],
      "metadata": {
        "id": "c24invNPaF32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# signal files are a bit smaller so we made this for convenience (uses more ram)\n",
        "def reduce_file_from_path(src, dst, name, percentage, f_and_w):\n",
        "    \"\"\"\n",
        "    src: source folder\n",
        "    dst: destination folder\n",
        "    name: name of the file when saved\n",
        "    percentage: % of the file you want\n",
        "    f_and_w: features and weights you want from the src file.\n",
        "\n",
        "    data is shuffeled before reduction\n",
        "  \"\"\"\n",
        "    new_signal = pd.read_hdf(src + name , 'mini')[f_and_w]\n",
        "    new_signal = shuffle(new_signal, random_state=42)\n",
        "    ZprimeX_sample = new_signal.sample(frac = percentage, random_state=42, replace = False)\n",
        "  \n",
        "    ZprimeX_sample.to_pickle(dst + name + '.pkl')\n",
        "\n"
      ],
      "metadata": {
        "id": "C7B0EzSPaTzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for signal_name in signal_name_list:\n",
        "  reduce_file_from_path(path, dst_path, signal_name, percentage, features_and_weights)"
      ],
      "metadata": {
        "id": "dxtvL5kWiTPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combines all signal files into a single file.\n",
        "signal_df = pd.DataFrame()\n",
        "for signal_name in signal_name_list:\n",
        "  new_signal = pd.read_hdf(path + signal_name, 'mini')[features_and_weights]\n",
        "  signal_df = signal_df.append(new_signal, ignore_index=True)"
      ],
      "metadata": {
        "id": "ll_m97QijI4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signal_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4qCxYgYmCYh",
        "outputId": "0786e380-40fb-4ea1-859c-200f0ad191cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(225998, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "signal_df = shuffle(signal_df, random_state=seed)"
      ],
      "metadata": {
        "id": "ffF64trxoaxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_file(signal_df, \"signal\", dst_path, percentage=signal_percentage)"
      ],
      "metadata": {
        "id": "ZJ9Vj-Pcmhn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# end"
      ],
      "metadata": {
        "id": "Boh2voZGnBBA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}